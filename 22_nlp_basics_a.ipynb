{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
      "metadata": {
        "id": "475819a4-e148-4616-b1cb-44b659aeb08a"
      },
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
      "metadata": {
        "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e"
      },
      "source": [
        "# NLP Basics\n",
        "\n",
        "**Transformers**\n",
        "\n",
        "&copy; Dr. Yves J. Hilpisch\n",
        "\n",
        "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b965031-7675-4fdc-907c-2a429eeb8778",
      "metadata": {
        "id": "9b965031-7675-4fdc-907c-2a429eeb8778"
      },
      "source": [
        "_Code primarily from ChatGPT_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120dab1b-8064-4b44-ac3a-4e74c2facae8",
      "metadata": {
        "id": "120dab1b-8064-4b44-ac3a-4e74c2facae8"
      },
      "source": [
        "## Transformer Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvAxwNoVq763"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!git clone https://github.com/tpq-classes/natural_language_processing.git\n",
        "import sys\n",
        "sys.path.append('natural_language_processing')\n"
      ],
      "id": "WvAxwNoVq763"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9232ad-a70a-4fec-873b-15ea751c3d0b",
      "metadata": {
        "id": "0c9232ad-a70a-4fec-873b-15ea751c3d0b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d086ac3-ccfa-4285-85d4-997a9ea82f94",
      "metadata": {
        "id": "0d086ac3-ccfa-4285-85d4-997a9ea82f94"
      },
      "outputs": [],
      "source": [
        "# Define a simple Transformer Encoder layer class\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer encoder layer.\n",
        "        - embed_dim: Dimension of the embedding space.\n",
        "        - num_heads: Number of attention heads.\n",
        "        - ff_dim: Hidden layer size in the feed-forward network.\n",
        "        - rate: Dropout rate to prevent overfitting.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Define the multi-head attention layer\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Define the feed-forward network: a two-layer MLP (Dense layers)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            # First dense layer with ReLU activation\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            # Second dense layer outputting the same dimensions as the input\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "        # Define layer normalization to stabilize training\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Define dropout layers to prevent overfitting\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass for the Transformer encoder.\n",
        "        - inputs: Input to the transformer encoder layer.\n",
        "        - training: Whether the layer is in training mode\n",
        "               (dropout applied) or inference mode.\n",
        "        \"\"\"\n",
        "        # Apply multi-head attention to the inputs (self-attention)\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # Add and normalize (residual connection and layer normalization)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # Add and normalize (residual connection and layer normalization)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18137026-c352-4fb1-97eb-c5664eb2508f",
      "metadata": {
        "id": "18137026-c352-4fb1-97eb-c5664eb2508f"
      },
      "source": [
        "## Real Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff7065a-ffd4-4108-9662-bfda1d6d5348",
      "metadata": {
        "id": "aff7065a-ffd4-4108-9662-bfda1d6d5348"
      },
      "outputs": [],
      "source": [
        "# Load the IMDb dataset from TensorFlow datasets\n",
        "imdb = tf.keras.datasets.imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b77ef1-4edb-4530-ae61-864f23654eff",
      "metadata": {
        "id": "a1b77ef1-4edb-4530-ae61-864f23654eff"
      },
      "outputs": [],
      "source": [
        "num_words = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025ce360-523e-489d-9aeb-da8cb1c60b9a",
      "metadata": {
        "id": "025ce360-523e-489d-9aeb-da8cb1c60b9a"
      },
      "outputs": [],
      "source": [
        "# Split into training and test datasets (X and y are reviews and labels, respectively)\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612a5422-975d-49a1-88f5-1afcb41278d4",
      "metadata": {
        "id": "612a5422-975d-49a1-88f5-1afcb41278d4"
      },
      "outputs": [],
      "source": [
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b38315-ce52-49ba-8100-d8a62ebda8df",
      "metadata": {
        "id": "56b38315-ce52-49ba-8100-d8a62ebda8df"
      },
      "outputs": [],
      "source": [
        "y_train[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98ee98d-3513-48cb-a56d-553bfaa52356",
      "metadata": {
        "id": "c98ee98d-3513-48cb-a56d-553bfaa52356"
      },
      "outputs": [],
      "source": [
        "N = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b3b98f-71d4-4add-a85d-8f8b176ec174",
      "metadata": {
        "id": "d3b3b98f-71d4-4add-a85d-8f8b176ec174"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[:N]\n",
        "y_train = y_train[:N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffcb752-cb74-4910-ac05-bdf0544ccd51",
      "metadata": {
        "id": "3ffcb752-cb74-4910-ac05-bdf0544ccd51"
      },
      "outputs": [],
      "source": [
        "X_test = X_test[:N]\n",
        "y_test = y_test[:N]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121b655a-b47f-44d6-9f10-0b1ae704a241",
      "metadata": {
        "id": "121b655a-b47f-44d6-9f10-0b1ae704a241"
      },
      "outputs": [],
      "source": [
        "# Maximum sequence length\n",
        "maxlen = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a2ba658-1ba7-49bd-95bf-288ed86b041f",
      "metadata": {
        "id": "3a2ba658-1ba7-49bd-95bf-288ed86b041f"
      },
      "outputs": [],
      "source": [
        "# Pad sequences to ensure uniform input length\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2278e234-75dc-418b-b94f-9d94c2b265c8",
      "metadata": {
        "id": "2278e234-75dc-418b-b94f-9d94c2b265c8"
      },
      "outputs": [],
      "source": [
        "n = 6\n",
        "X_train[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477ec4b6-ed8a-4c4e-a876-ad7bc6efe8cd",
      "metadata": {
        "id": "477ec4b6-ed8a-4c4e-a876-ad7bc6efe8cd"
      },
      "outputs": [],
      "source": [
        "y_train[n]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "506b1021-14a8-42c3-a7f8-3e2d6c482df3",
      "metadata": {
        "id": "506b1021-14a8-42c3-a7f8-3e2d6c482df3"
      },
      "source": [
        "## Transformer Example (1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbebba8e-7548-40b0-9b0e-28705ff0ff37",
      "metadata": {
        "id": "dbebba8e-7548-40b0-9b0e-28705ff0ff37"
      },
      "source": [
        "**Text Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a161f0f-5b45-4064-b103-4a353e6df352",
      "metadata": {
        "id": "8a161f0f-5b45-4064-b103-4a353e6df352"
      },
      "outputs": [],
      "source": [
        "# Define a Transformer-based text classification model\n",
        "def create_transformer_model(input_shape, embed_dim,\n",
        "                             num_heads, ff_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Create a Transformer-based classification model.\n",
        "    - input_shape: Shape of the input data\n",
        "        (number of tokens in each sequence).\n",
        "    - embed_dim: Dimension of the embedding.\n",
        "    - num_heads: Number of attention heads in the Transformer encoder.\n",
        "    - ff_dim: Feed-forward network dimension.\n",
        "    - num_classes: Number of output classes for classification.\n",
        "    \"\"\"\n",
        "    # Define the input layer. Expect sequences of integers (token IDs)\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Embed the input tokens using an embedding layer\n",
        "    x = layers.Embedding(input_dim=num_words, output_dim=embed_dim)(inputs)\n",
        "\n",
        "    # Pass the embeddings through the Transformer encoder layer\n",
        "    x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Apply global average pooling to reduce the sequence to a\n",
        "    # fixed size (averaging across tokens)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Add a dense output layer with softmax activation for classification\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the Keras model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa66ced-84c9-4d90-acb4-0cfa1045eb7d",
      "metadata": {
        "id": "aaa66ced-84c9-4d90-acb4-0cfa1045eb7d"
      },
      "source": [
        "## Transformer Application (1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "292dcd22-46a9-47ff-a5f2-ce47f274dd0e",
      "metadata": {
        "id": "292dcd22-46a9-47ff-a5f2-ce47f274dd0e"
      },
      "source": [
        "**Text Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a94171-dfae-4a7f-80a9-22e94466488f",
      "metadata": {
        "id": "02a94171-dfae-4a7f-80a9-22e94466488f"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e13b7e-4d0a-49b4-822e-1e881c09fe6b",
      "metadata": {
        "id": "d5e13b7e-4d0a-49b4-822e-1e881c09fe6b"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "embed_dim = 64  # Size of the token embeddings\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 128  # Hidden layer size in the feed-forward network\n",
        "num_classes = 2  # Number of output classes (for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ed19ca-561f-4353-bf62-0607b44877bb",
      "metadata": {
        "id": "91ed19ca-561f-4353-bf62-0607b44877bb"
      },
      "outputs": [],
      "source": [
        "# Create the model using the function defined above\n",
        "model = create_transformer_model(input_shape=(maxlen,),\n",
        "            embed_dim=embed_dim, num_heads=num_heads,\n",
        "            ff_dim=ff_dim, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b9c07a-bf2c-4f1b-bd27-7a3d52e842c9",
      "metadata": {
        "id": "32b9c07a-bf2c-4f1b-bd27-7a3d52e842c9"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer,\n",
        "# sparse categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7513450-b04d-4f23-b4fe-959574145b41",
      "metadata": {
        "id": "b7513450-b04d-4f23-b4fe-959574145b41"
      },
      "outputs": [],
      "source": [
        "# Print the model summary to visualize the architecture\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2639333d-46db-4a73-9fbc-6582867f714e",
      "metadata": {
        "id": "2639333d-46db-4a73-9fbc-6582867f714e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Train the model with the training dataset\n",
        "history = model.fit(X_train, y_train, epochs=35,\n",
        "                    batch_size=64, validation_split=0.2,\n",
        "                   verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d0889-72dd-40e2-9976-19f9803f163a",
      "metadata": {
        "id": "e84d0889-72dd-40e2-9976-19f9803f163a"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea8fae1-d540-4a0f-995b-3c9cadd9bd36",
      "metadata": {
        "id": "aea8fae1-d540-4a0f-995b-3c9cadd9bd36"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cab6ac3-c5e7-4c68-b7ff-a6c6c63942c1",
      "metadata": {
        "id": "9cab6ac3-c5e7-4c68-b7ff-a6c6c63942c1"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f875510-b1b3-4caf-899d-b48e3e09924e",
      "metadata": {
        "id": "3f875510-b1b3-4caf-899d-b48e3e09924e"
      },
      "outputs": [],
      "source": [
        "# Load the word index used by the IMDb dataset\n",
        "word_index = imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc75d2d-44c8-4286-9bf8-b16c4d46636a",
      "metadata": {
        "id": "3dc75d2d-44c8-4286-9bf8-b16c4d46636a"
      },
      "outputs": [],
      "source": [
        "# word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf5a07d-720f-4508-a982-6687895dbf33",
      "metadata": {
        "id": "caf5a07d-720f-4508-a982-6687895dbf33"
      },
      "outputs": [],
      "source": [
        "# Reverse the word index to get the word from the integer (to ensure we use the 5000 top words)\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}  # Shift by 3 to account for reserved indices\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # Unknown words\n",
        "word_index[\"<UNUSED>\"] = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5023848-1dd0-4282-bb16-67918bc75188",
      "metadata": {
        "id": "d5023848-1dd0-4282-bb16-67918bc75188"
      },
      "outputs": [],
      "source": [
        "# Limit the word index to num_words words (top most frequent)\n",
        "word_index = {k: v for k, v in word_index.items() if v < num_words}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9842a6d-4818-41fa-96c3-153247a4c9d2",
      "metadata": {
        "id": "a9842a6d-4818-41fa-96c3-153247a4c9d2"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess a single input review text\n",
        "def preprocess_text(text, word_index, maxlen=100):\n",
        "    # Tokenize the text based on the top 5000 words in the IMDb word index\n",
        "    tokens = []\n",
        "    for word in text.lower().split():\n",
        "        # Map word to token, if word is outside the top 5000 words, map to <UNK> (index 2)\n",
        "        token = word_index.get(word, 2)  # Use 2 for unknown words\n",
        "        tokens.append(token)\n",
        "\n",
        "    # Pad the sequence to the maximum length\n",
        "    padded_seq = pad_sequences([tokens], padding='post', maxlen=maxlen)\n",
        "    return padded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e6616e-a186-4b8c-9d39-a0760904a630",
      "metadata": {
        "id": "a9e6616e-a186-4b8c-9d39-a0760904a630"
      },
      "outputs": [],
      "source": [
        "# Test with a new sample review\n",
        "sample_review = \"This movie was fantastic, I loved it.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ebdb0b-c999-4677-aa8a-32640210012a",
      "metadata": {
        "id": "84ebdb0b-c999-4677-aa8a-32640210012a"
      },
      "outputs": [],
      "source": [
        "sample_review = \"The movie was not good.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa1dfeb-f2a7-47ea-8919-3bbf338ab6b2",
      "metadata": {
        "id": "8fa1dfeb-f2a7-47ea-8919-3bbf338ab6b2"
      },
      "outputs": [],
      "source": [
        "sample_input = preprocess_text(3 * sample_review, word_index)\n",
        "sample_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d65c010-821b-43cd-8388-a2bf54421f57",
      "metadata": {
        "id": "3d65c010-821b-43cd-8388-a2bf54421f57"
      },
      "outputs": [],
      "source": [
        "# Predict the sentiment of the sample review\n",
        "prediction = model.predict(sample_input)\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c06eb3-f660-4de5-80e0-af5fd48a4b65",
      "metadata": {
        "id": "b1c06eb3-f660-4de5-80e0-af5fd48a4b65"
      },
      "outputs": [],
      "source": [
        "predicted_class = np.argmax(prediction, axis=1)\n",
        "predicted_class[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18850af-0a03-42ed-a9be-e1a0787b2116",
      "metadata": {
        "id": "c18850af-0a03-42ed-a9be-e1a0787b2116"
      },
      "outputs": [],
      "source": [
        "# Output the predicted class (0 = negative, 1 = positive)\n",
        "print(f'Predicted class: {predicted_class[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
      "metadata": {
        "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55"
      },
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
        "\n",
        "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}