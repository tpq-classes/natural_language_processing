{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "**Word Embeddings**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d071354-f0e1-4bf8-9917-020d62a000d0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/tpq-classes/natural_language_processing.git\n",
    "import sys\n",
    "sys.path.append('natural_language_processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032f0cb-8e91-409a-9c62-6839055c64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8949cc1-4d01-45ce-ba5d-b05fd83b6de4",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ce126-8a06-4d83-a9e5-60cfcaa98512",
   "metadata": {},
   "source": [
    "_From ChatGPT._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca0e0d-ebf5-4ef2-ab9e-6a5968cdab50",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique used in natural language processing (NLP) to create word embeddings, which are dense vector representations of words. Developed by a team of researchers at Google led by Tomas Mikolov in 2013, Word2Vec models learn to map words into high-dimensional continuous vector spaces where semantically similar words are located close to each other.\n",
    "\n",
    "### Key Concepts of Word2Vec\n",
    "\n",
    "1. **Distributed Representations**: Unlike traditional one-hot encoding, which represents words as sparse vectors with many dimensions (one per unique word) and no meaningful distances between them, Word2Vec creates dense vectors where the semantic relationships between words are captured in the vector space.\n",
    "\n",
    "2. **Training Approaches**:\n",
    "   - **Continuous Bag of Words (CBOW)**: Predicts the target word based on the context of surrounding words. It uses a window of words around the target word to predict the target word itself.\n",
    "   - **Skip-gram**: Predicts the surrounding context words based on the target word. Given a word, it tries to predict the words in its neighborhood.\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "1. **Input**: A large corpus of text.\n",
    "2. **Training**: The model is trained on the corpus to predict context words from a target word (skip-gram) or a target word from context words (CBOW).\n",
    "3. **Output**: A set of word vectors where each word is represented by a dense vector of real numbers.\n",
    "\n",
    "### Benefits of Word2Vec\n",
    "\n",
    "- **Semantic Relationships**: Words with similar meanings are close together in the vector space. For example, \"king\" - \"man\" + \"woman\" is close to \"queen\".\n",
    "- **Efficient**: Word2Vec can be trained on large corpora efficiently using stochastic gradient descent and other optimization techniques.\n",
    "- **Generalization**: The vectors can be used in various downstream NLP tasks, improving their performance by providing meaningful word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3753c-dc74-4be3-bee5-32079e3acf67",
   "metadata": {},
   "source": [
    "### Example in Python using Gensim\n",
    "\n",
    "Here's how you can use the Gensim library to create Word2Vec embeddings:\n",
    "\n",
    "1. **Install Gensim**:\n",
    "   ```bash\n",
    "   pip install gensim\n",
    "   ```\n",
    "\n",
    "2. **Train a Word2Vec Model**:\n",
    "   ```python\n",
    "   from gensim.models import Word2Vec\n",
    "\n",
    "   # Sample sentences\n",
    "   sentences = [\n",
    "       [\"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "       [\"dog\", \"barked\", \"at\", \"the\", \"mailman\"],\n",
    "       [\"fish\", \"swims\", \"in\", \"the\", \"water\"]\n",
    "   ]\n",
    "\n",
    "   # Train Word2Vec model\n",
    "   model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "   # Get embeddings for a word\n",
    "   cat_vector = model.wv['cat']\n",
    "   print(\"Embedding for 'cat':\", cat_vector)\n",
    "   \n",
    "   # Find similar words\n",
    "   similar_to_cat = model.wv.most_similar('cat')\n",
    "   print(\"Words similar to 'cat':\", similar_to_cat)\n",
    "   ```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Training Data**: We use a small set of sample sentences.\n",
    "- **Word2Vec Model**: We create a Word2Vec model using these sentences.\n",
    "  - `vector_size`: The size of the word vectors.\n",
    "  - `window`: The maximum distance between the current and predicted word within a sentence.\n",
    "  - `min_count`: Ignores all words with a total frequency lower than this.\n",
    "  - `sg`: Training algorithm, 0 for CBOW (Continuous Bag of Words), and 1 for skip-gram.\n",
    "- **Get Embeddings**: We retrieve the embeddings for specific words like 'cat'.\n",
    "- **Find Similar Words**: We find words similar to 'cat' based on the trained embeddings.\n",
    "\n",
    "### Applications of Word2Vec\n",
    "\n",
    "1. **Text Classification**: Improved feature representations lead to better classification performance.\n",
    "2. **Semantic Analysis**: Understanding relationships between words.\n",
    "3. **Machine Translation**: Capturing the semantic meaning of words helps in translating sentences more accurately.\n",
    "4. **Recommendation Systems**: Finding similar items or content based on word embeddings.\n",
    "5. **Information Retrieval**: Improving search results by understanding the semantic context of queries.\n",
    "\n",
    "Word2Vec has been a foundational technique in NLP, leading to more advanced embedding methods like GloVe, FastText, and contextual embeddings such as BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b1b82-bab7-4f8f-a7a0-33989573152a",
   "metadata": {},
   "source": [
    "[Note: `gensim` currently requires `scipy` version 1.12 (does not work with 1.13 anymore).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d92227-50e2-41f1-9188-65c753ff090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cea73-30fc-4910-90ea-4bf7ccc1893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "   [\"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "   [\"dog\", \"barked\", \"at\", \"the\", \"mailman\"],\n",
    "   [\"fish\", \"swims\", \"in\", \"the\", \"water\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c9d2a-f93c-4c37-b96d-b84c97be41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=3,\n",
    "                 window=2, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e672b15-b32b-4847-8066-2c0a50f9f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for a word\n",
    "cat_vector = model.wv['cat']\n",
    "print(\"Embedding for 'cat':\", cat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cbb3f-4d34-45f9-86ef-85786203715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "similar_to_cat = model.wv.most_similar('cat')\n",
    "print(\"Words similar to 'cat':\", similar_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a208478-5de6-4bce-a40d-ae5a2dba8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for a word\n",
    "dog_vector = model.wv['dog']\n",
    "print(\"Embedding for 'dog':\", dog_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f797d3a-3869-4d20-b2f1-762f95676ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "similar_to_dog = model.wv.most_similar('dog')\n",
    "print(\"Words similar to 'dog':\", similar_to_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaac2e-64c6-4733-abeb-c7d07558955d",
   "metadata": {},
   "source": [
    "## Distance and Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470035df-27ee-4dfa-9053-ecabd52adb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_snippets = [\n",
    "    \"Python is a versatile language for web development, data analysis, and automation.\",\n",
    "    \"Use Python's libraries like NumPy and Pandas for efficient data manipulation.\",\n",
    "    \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n",
    "    \"The Python community offers extensive documentation and a wealth of online resources.\",\n",
    "    \"Python's syntax is designed to be readable and straightforward, making it beginner-friendly.\",\n",
    "    \"Django and Flask are popular frameworks for developing web applications in Python.\",\n",
    "    \"Automate repetitive tasks with Python scripts and save time in your workflow.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa1b83-d82e-4cb2-bfc5-fb28e8ba8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_snippets = [\n",
    "    \"Natural Language Processing (NLP) enables computers to understand and process human language.\",\n",
    "    \"NLP is used in applications like sentiment analysis, chatbots, and machine translation.\",\n",
    "    \"Tokenization is a fundamental step in NLP, breaking text into meaningful units.\",\n",
    "    \"Named Entity Recognition (NER) identifies proper nouns in text, such as names and locations.\",\n",
    "    \"Vectorization converts text data into numerical form for machine learning models.\",\n",
    "    \"Popular NLP libraries include NLTK, SpaCy, and Hugging Face Transformers.\",\n",
    "    \"NLP combines computational linguistics and machine learning for language understanding.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bbeee-da45-4967-8beb-e4380355f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_snippets = [\n",
    "    \"Large Language Models (LLMs) are advanced neural networks trained on vast text corpora.\",\n",
    "    \"LLMs like GPT-3 generate human-like text based on input prompts.\",\n",
    "    \"Applications of LLMs include content creation, code generation, and conversational agents.\",\n",
    "    \"LLMs utilize transformers, a deep learning architecture, for efficient processing.\",\n",
    "    \"Training LLMs requires substantial computational resources and large datasets.\",\n",
    "    \"Fine-tuning LLMs on specific tasks enhances their performance and accuracy.\",\n",
    "    \"Ethical considerations in LLMs include bias, misinformation, and data privacy.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f75d3-b6cb-4138-b9ec-fc031757394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "X.extend(python_snippets)\n",
    "X.extend(nlp_snippets)\n",
    "X.extend(llm_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35d163-3099-490c-9bdd-673f996b9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [s.lower() for s in X]\n",
    "X = [s.split() for s in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f37cb-a458-4180-afdd-eec30ba1cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for s in X:\n",
    "    sentences.append([w.strip('.,()') for w in s])\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd1c27-89d6-4777-993e-9de62c1d53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1,\n",
    "                 vector_size=5, sg=0, window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879885e-df60-4ff7-906e-239473863c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94460198-3291-408a-aca9-380ebaba1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['django']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fb039-468f-43a3-b063-81164d113f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f0d02-2ad2-4c41-81bd-8c34f0d818bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('python', 'django')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a77a5-2e68-4340-92c7-5d9dadd203c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('python', 'chatbots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b4064-b579-4314-9f53-83842e5608d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('django', 'chatbots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3da78-ab94-4f52-a0cd-231404c13034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53929641-ab2a-490e-b2e7-c39a7ef85697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('django')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c0cc8-90dc-449e-a106-49bb1825033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('chatbots')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380ab80-ac7d-4130-800d-8946196dbcae",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4ddd2-0dbf-441a-b30b-38ece00dbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = ' '.join(python_snippets).lower()\n",
    "d2 = ' '.join(nlp_snippets).lower()\n",
    "d3 = ' '.join(llm_snippets).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93ea9b-05c6-43ad-b977-2b423b92cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f0d26-3735-4e94-8ead-f73dcaa9645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tokens = [w.strip('.,()') for w in d1.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d7a2b-d6d3-494b-875f-8dbf14b584a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_tokens[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93baa45f-c174-46ad-99d5-432645602626",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dc8ce-6cb0-4eb1-a53e-efdc40c65e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.wv[t] for t in d1_tokens][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5071b5-9b1d-4acc-b1bd-875d2461ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_vec = np.mean([model.wv[t] for t in d1_tokens], axis=0)\n",
    "d1_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604688af-0091-4fbc-9948-0f0f9d822a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(doc):\n",
    "    doc_tokens = [w.strip('.,()') for w in doc.split()]\n",
    "    dv = np.mean([model.wv[t] for t in doc_tokens], axis=0)\n",
    "    return dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406be9a-9a03-47e2-965e-1afd6701d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_vec = get_doc_vector(d2)\n",
    "d2_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e141af4-cd46-4688-bc08-d579846c8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3_vec = get_doc_vector(d3)\n",
    "d3_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db973c6-8098-4ce5-b4ea-620ecb285ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.cosine_similarities(d1_vec, [d2_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5546e-808a-47ce-9a15-35eda0418c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.cosine_similarities(d1_vec, [d3_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbffd4a-5a35-46ad-b004-bb8744a3bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.cosine_similarities(d2_vec, [d3_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcbab8-7fe1-4dec-91d9-059f5e993ba0",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f752d-cbba-4b90-b96d-618625df0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ = [\n",
    "    'i am so happy today',\n",
    "    'i hate rainy days',\n",
    "    'i love this book',\n",
    "    'i hate boring movies',\n",
    "    'i hope for the best',\n",
    "    'i fear the worst'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68769ae-2133-49ad-9ffc-889e74946615",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = np.array((1, 0, 1, 0, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59215627-56ff-4634-998b-c371e2e1fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s.split() for s in sentences_]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc937ed5-7767-40df-bad1-584e4529332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1,\n",
    "                 vector_size=5, window=2, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5d3f0-c2cb-42a1-bd5d-74ec4e3557ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs = [get_doc_vector(s) for s in sentences_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5003d54-d2d3-4aca-9b6a-75ecf2e3dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377d757-39be-4d69-ba2b-b804e711fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909cd8b-fbad-4fcd-a4e8-4c7f4f1d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e32c0-e773-4ec8-b912-bd787d1074b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(sent_vecs, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec8442-0e70-48e3-9518-2e516f6fb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(sent_vecs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52f1bc-a778-4841-8ab6-6f113cff2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(sent_vecs) == sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7592084-f6a6-4fef-814f-63f0d45a00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    'i love movies',\n",
    "    'i hate today',\n",
    "    'i fear rainy days',\n",
    "    'i hope for happy days'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35f94e-e145-431b-aca7-e3eef14980c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = [get_doc_vector(s) for s in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e03ec-63fd-4d00-a3c1-982a03c10dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752d038-8dc4-4704-bf4a-730780591a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07215a-0184-4610-a456-0ee8e1fd2a89",
   "metadata": {},
   "source": [
    "## APPENDIX: Word2Vec Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c01829-5a3a-45a8-9511-e39aa386055b",
   "metadata": {},
   "source": [
    "_From ChatGPT._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f955a7-9bc6-4491-b074-0ecdf2502b50",
   "metadata": {},
   "source": [
    "Word2Vec uses two main algorithms to learn vector representations of words: **Continuous Bag of Words (CBOW)** and **Skip-gram**. Both of these are neural network models designed to predict either a word given its context (CBOW) or the context given a word (Skip-gram).\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "\n",
    "In CBOW, the model predicts the target word (center word) based on the context words (surrounding words). \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a simple sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "- Context window size: 2 (consider 2 words before and 2 words after the target word)\n",
    "\n",
    "For the word \"sat\" (target), the context words are [\"The\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "The CBOW model tries to predict \"sat\" from [\"The\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "In Skip-gram, the model predicts the context words given a target word (center word).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's use the same sentence: \"The cat sat on the mat.\"\n",
    "\n",
    "- Context window size: 2\n",
    "\n",
    "For the word \"sat\" (target), the model will try to predict [\"The\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "### How the Algorithm Works\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize the weights of the neural network randomly.\n",
    "\n",
    "2. **Training:**\n",
    "   - For each word in the vocabulary, use the word and its context to update the neural network weights.\n",
    "   - The training process involves:\n",
    "     - Converting words into one-hot vectors.\n",
    "     - Feeding these one-hot vectors into the neural network.\n",
    "     - Calculating the error between the predicted word(s) and the actual word(s).\n",
    "     - Backpropagating the error to adjust the weights.\n",
    "\n",
    "### Simple Example with Skip-gram\n",
    "\n",
    "Consider the sentence: \"I like cats.\"\n",
    "\n",
    "- Vocabulary: [\"I\", \"like\", \"cats\"]\n",
    "- Context window size: 1\n",
    "\n",
    "For the target word \"like\":\n",
    "- Context words: [\"I\", \"cats\"]\n",
    "\n",
    "The Skip-gram model will generate training pairs: \n",
    "- (\"like\", \"I\")\n",
    "- (\"like\", \"cats\")\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **One-hot Encoding:**\n",
    "   - \"like\": [0, 1, 0]\n",
    "   - \"I\": [1, 0, 0]\n",
    "   - \"cats\": [0, 0, 1]\n",
    "\n",
    "2. **Neural Network Structure:**\n",
    "   - Input layer: One-hot vector of the target word (e.g., \"like\" -> [0, 1, 0])\n",
    "   - Hidden layer: A weight matrix that transforms the one-hot vector into a dense vector.\n",
    "   - Output layer: A weight matrix that transforms the dense vector back into the vocabulary space.\n",
    "\n",
    "3. **Forward Pass:**\n",
    "   - Input: [0, 1, 0] (one-hot for \"like\")\n",
    "   - Hidden layer: Multiply the input with the weight matrix to get the hidden representation (embedding).\n",
    "   - Output layer: Multiply the hidden representation with the weight matrix to get the scores for each word in the vocabulary.\n",
    "\n",
    "4. **Prediction and Error Calculation:**\n",
    "   - Apply softmax to get the probability distribution over the vocabulary.\n",
    "   - Calculate the error based on the actual context words.\n",
    "\n",
    "5. **Backpropagation:**\n",
    "   - Update the weights based on the error using gradient descent.\n",
    "\n",
    "After training, the hidden layer weights will contain the word embeddings. Words that appear in similar contexts will have similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec2282-0ab0-43b5-8ed1-66fd7d9c901f",
   "metadata": {},
   "source": [
    "## APPENDIX: Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614f573-070f-4a7d-a65a-281344f62f2e",
   "metadata": {},
   "source": [
    "Cosine similarity is a measure that calculates the cosine of the angle between two non-zero vectors in an inner product space. It is often used to measure the similarity between two vectors, particularly in the context of document similarity and word embeddings.\n",
    "\n",
    "### Cosine Similarity Formula\n",
    "\n",
    "The cosine similarity between two vectors $A$ and $B$ is calculated as:\n",
    "\n",
    "$$ \\text{Cosine Similarity} = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $$\n",
    "\n",
    "where:\n",
    "- $ A \\cdot B $ is the dot product of vectors $A$ and $B$.\n",
    "- $\\|A\\|$ is the magnitude (length) of vector $A$.\n",
    "- $\\|B\\|$ is the magnitude (length) of vector $B$.\n",
    "- $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "### Step-by-Step Calculation\n",
    "\n",
    "1. **Dot Product:**\n",
    "   The dot product of two vectors $A$ and $B$, each of dimension $n$, is calculated as:\n",
    "   \n",
    "   $$ A \\cdot B = \\sum_{i=1}^{n} A_i \\times B_i $$\n",
    "\n",
    "2. **Magnitude of Vectors:**\n",
    "   The magnitude of a vector $A$ is calculated as:\n",
    "   \n",
    "   $$ \\|A\\| = \\sqrt{\\sum_{i=1}^{n} A_i^2} $$\n",
    "\n",
    "   Similarly, the magnitude of vector $B$ is:\n",
    "   \n",
    "   $$ \\|B\\| = \\sqrt{\\sum_{i=1}^{n} B_i^2} $$\n",
    "\n",
    "3. **Cosine Similarity Calculation:**\n",
    "   Combine the dot product and magnitudes to get the cosine similarity:\n",
    "   \n",
    "   $$ \\cos(\\theta) = \\frac{\\sum_{i=1}^{n} A_i \\times B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Let's consider two vectors $A$ and $B$:\n",
    "\n",
    "$$ A = [1, 2, 3] $$\n",
    "$$ B = [4, 5, 6] $$\n",
    "\n",
    "1. **Dot Product:**\n",
    "   \n",
    "   $$ A \\cdot B = (1 \\times 4) + (2 \\times 5) + (3 \\times 6) = 4 + 10 + 18 = 32 $$\n",
    "\n",
    "2. **Magnitude of Vectors:**\n",
    "\n",
    "   $$ \\|A\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14} \\approx 3.74 $$\n",
    "   \n",
    "   $$ \\|B\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{16 + 25 + 36} = \\sqrt{77} \\approx 8.77 $$\n",
    "\n",
    "3. **Cosine Similarity:**\n",
    "\n",
    "   $$ \\cos(\\theta) = \\frac{32}{3.74 \\times 8.77} = \\frac{32}{32.8} \\approx 0.98 $$\n",
    "\n",
    "The cosine similarity between vectors $A$ and $B$ is approximately 0.98, indicating a high degree of similarity.\n",
    "\n",
    "### Python Code Example\n",
    "\n",
    "Here's how you can calculate cosine similarity using Python and the `numpy` library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define vectors\n",
    "A = np.array([1, 2, 3])\n",
    "B = np.array([4, 5, 6])\n",
    "\n",
    "# Calculate dot product\n",
    "dot_product = np.dot(A, B)\n",
    "\n",
    "# Calculate magnitudes\n",
    "magnitude_A = np.linalg.norm(A)\n",
    "magnitude_B = np.linalg.norm(B)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "print(\"Cosine Similarity:\", cosine_similarity)\n",
    "```\n",
    "\n",
    "### Applications\n",
    "\n",
    "Cosine similarity is widely used in:\n",
    "- **Information Retrieval:** To measure the similarity between documents.\n",
    "- **Text Mining:** To find similar texts.\n",
    "- **Recommender Systems:** To recommend items that are similar to user preferences.\n",
    "- **Word Embeddings:** To measure the similarity between word vectors in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}