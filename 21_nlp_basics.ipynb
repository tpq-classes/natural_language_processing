{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
      "metadata": {
        "id": "475819a4-e148-4616-b1cb-44b659aeb08a"
      },
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
      "metadata": {
        "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e"
      },
      "source": [
        "# NLP Basics\n",
        "\n",
        "**Transformers**\n",
        "\n",
        "&copy; Dr. Yves J. Hilpisch\n",
        "\n",
        "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85fc6a1d-c5d7-4da7-895b-f040643566bd",
      "metadata": {
        "id": "85fc6a1d-c5d7-4da7-895b-f040643566bd"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad682d-bc3f-426a-bdb1-4c9c25a522cb",
      "metadata": {
        "id": "13ad682d-bc3f-426a-bdb1-4c9c25a522cb"
      },
      "source": [
        "The following is a concise **review** of the self-attention mechanism and its interpretation. Example from ChatGPT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTY4-crqnAa-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!git clone https://github.com/tpq-classes/natural_language_processing.git\n",
        "import sys\n",
        "sys.path.append('natural_language_processing')\n"
      ],
      "id": "CTY4-crqnAa-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8f6eec-d27e-4f1f-8ff8-fdf9d2992c7d",
      "metadata": {
        "id": "4d8f6eec-d27e-4f1f-8ff8-fdf9d2992c7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8b7c0d-341e-45ac-8e98-78d931c3b03c",
      "metadata": {
        "id": "0c8b7c0d-341e-45ac-8e98-78d931c3b03c"
      },
      "outputs": [],
      "source": [
        "# Step 1: Example text (simple sentences)\n",
        "texts = [\"The cat sat on the mat\",\n",
        "         \"The mat was sat on by the cat\",\n",
        "         \"The cat is playing with a ball\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48ca64a-8c58-4ad9-ae6e-e2572bafe382",
      "metadata": {
        "id": "d48ca64a-8c58-4ad9-ae6e-e2572bafe382"
      },
      "outputs": [],
      "source": [
        "# Step 2: Generate embeddings using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_embeddings = vectorizer.fit_transform(texts).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972e54a5-94d7-440b-b758-49606fd8595b",
      "metadata": {
        "id": "972e54a5-94d7-440b-b758-49606fd8595b"
      },
      "outputs": [],
      "source": [
        "# Display the feature names and TF-IDF embeddings\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Embeddings:\\n\", tfidf_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b030b07-c73f-413a-926c-ea5b67e20fa6",
      "metadata": {
        "id": "3b030b07-c73f-413a-926c-ea5b67e20fa6"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define a simple self-attention mechanism\n",
        "def self_attention(embeddings):\n",
        "    \"\"\"\n",
        "    Calculate attention scores for the input embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 4: Compute the attention scores\n",
        "    # Self-attention is often computed using: attention = Q * K.T\n",
        "    # For simplicity, we use embeddings as both queries (Q) and keys (K)\n",
        "    attention_scores = np.dot(embeddings, embeddings.T)\n",
        "\n",
        "    # Step 5: Apply softmax to normalize the attention scores (weights)\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "    attention_weights = softmax(attention_scores)\n",
        "\n",
        "    # Step 6: Multiply the weights with the value (same as embeddings here)\n",
        "    # Contextualized embeddings (values) = attention_weights * values (here embeddings)\n",
        "    context_embeddings = np.dot(attention_weights, embeddings)\n",
        "\n",
        "    return attention_scores, attention_weights, context_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac6ebc2-b3d6-4f5a-b438-1a315776cec2",
      "metadata": {
        "id": "6ac6ebc2-b3d6-4f5a-b438-1a315776cec2"
      },
      "outputs": [],
      "source": [
        "# Step 7: Calculate self-attention on the embeddings\n",
        "attention_scores, attention_weights, context_embeddings = self_attention(tfidf_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "081ec371-605d-4708-b636-0d9033e26f3b",
      "metadata": {
        "id": "081ec371-605d-4708-b636-0d9033e26f3b"
      },
      "source": [
        "#### Explanation of the Code:\n",
        "\n",
        "1. **Text Data (`texts`)**: We start with a small list of three simple sentences to illustrate how self-attention works on textual data.\n",
        "\n",
        "2. **TF-IDF Vectorization**:\n",
        "   - We use the `TfidfVectorizer` from `sklearn` to create embeddings for each sentence.\n",
        "   - The `fit_transform` method generates TF-IDF scores for each word, resulting in numerical vectors representing the importance of words in each sentence.\n",
        "   - The `tfidf_embeddings` is a matrix where each row is the vector representation of a sentence.\n",
        "\n",
        "3. **Self-Attention Function (`self_attention`)**:\n",
        "   - **Attention Scores**: We compute the dot product of the embeddings to get the attention scores. These represent how much focus each sentence should pay to the others. Here, the sentences themselves serve as both the query and key.\n",
        "   - **Softmax**: We apply softmax to the attention scores to convert them into probabilities (attention weights). This ensures that the weights across sentences sum to 1.\n",
        "   - **Contextualized Embeddings**: Finally, we compute the new embeddings by multiplying the attention weights with the original embeddings. This generates \"contextualized\" representations of each sentence, influenced by the other sentences.\n",
        "\n",
        "4. **Interpretation of Results**:\n",
        "   - **Attention Scores**: These raw scores indicate how much one sentence relates to another. High scores indicate higher similarity.\n",
        "   - **Attention Weights**: After applying softmax, these normalized scores tell us how much attention one sentence pays to another (normalized to probabilities).\n",
        "   - **Contextualized Embeddings**: These embeddings represent the sentences after the attention mechanism. Each sentence is now a weighted sum of all the original sentences, capturing the context from the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a624724-d83e-4f5b-8b12-a321b8e6a1e7",
      "metadata": {
        "id": "7a624724-d83e-4f5b-8b12-a321b8e6a1e7"
      },
      "outputs": [],
      "source": [
        "# Step 8: Interpret the results\n",
        "print(\"\\nAttention Scores (before softmax):\\n\", attention_scores)\n",
        "print(\"\\nAttention Weights (after softmax):\\n\", attention_weights)\n",
        "print(\"\\nContextualized Embeddings:\\n\", context_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f062e3c-0d39-421b-9bd3-959d2bb0b0e2",
      "metadata": {
        "id": "4f062e3c-0d39-421b-9bd3-959d2bb0b0e2"
      },
      "source": [
        "### Key Interpretations:\n",
        "- **Attention Scores**: The diagonal entries (1.0) represent each sentence's self-attention, while the off-diagonal values (like 0.7968) represent how similar the sentences are to one another.\n",
        "- **Attention Weights**: These are the normalized attention scores, indicating how much attention each sentence pays to others. For example, sentence 1 pays about 44% attention to itself and 36% to sentence 2.\n",
        "- **Contextualized Embeddings**: Each sentence now incorporates information from the other sentences, with the weights applied based on attention. For example, sentence 1's new representation is a mix of itself (44%), sentence 2 (36%), and sentence 3 (21%).\n",
        "\n",
        "This is a basic illustration, but it shows how self-attention works using simple text and embeddings from TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9380ab80-ac7d-4130-800d-8946196dbcae",
      "metadata": {
        "id": "9380ab80-ac7d-4130-800d-8946196dbcae"
      },
      "source": [
        "## Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd15a0a1-d58d-4c98-840c-07981bf64604",
      "metadata": {
        "id": "cd15a0a1-d58d-4c98-840c-07981bf64604"
      },
      "source": [
        "_From ChatGPT_:\n",
        "\n",
        "The relationship between **embeddings**, **self-attention**, and **transformers** is central to the functioning of modern natural language processing (NLP) architectures. Here’s a detailed breakdown of how they interrelate:\n",
        "\n",
        "### 1. **Embeddings (e.g. Word2Vec)**\n",
        "\n",
        "**Embeddings** are a technique to represent words or tokens in a continuous vector space. The core idea is to encode words into fixed-length dense vectors, where words with similar meanings are placed closer together in this vector space.\n",
        "\n",
        "- **Word2Vec** is an early and widely-used embedding technique that creates word vectors based on their context (Skip-gram or CBOW). The vectors capture semantic relationships between words. For instance, the vector difference between \"king\" and \"queen\" would be similar to that between \"man\" and \"woman.\"\n",
        "  \n",
        "- In modern NLP models (including transformers), embeddings serve as the **input representations** of the words. The input to these models is typically a sequence of embeddings, where each embedding corresponds to a word or sub-word token.\n",
        "\n",
        "However, **Word2Vec** is static, meaning each word has a single vector regardless of context. In contrast, newer models like **BERT** (a transformer model) produce **contextual embeddings** where the meaning of a word can vary depending on its usage.\n",
        "\n",
        "### 2. **Self-Attention Mechanism**\n",
        "\n",
        "The **self-attention** mechanism is a core building block of the transformer architecture and is a way of enabling the model to focus on different parts of a sequence when making predictions about a particular token.\n",
        "\n",
        "- In self-attention, each token in a sequence interacts with every other token, learning **which parts of the sequence are important** to focus on. The model computes a weighted sum of all the tokens, where the weights are dynamically determined by how relevant each token is to the one being processed.\n",
        "\n",
        "  This is done using three vectors for each token:\n",
        "  - **Query (Q)**: What are we looking for in the sequence?\n",
        "  - **Key (K)**: What information does each token have?\n",
        "  - **Value (V)**: What is the actual information that each token holds?\n",
        "\n",
        "The attention score for a token is calculated by taking the dot product of the query vector of the current token with the key vectors of all tokens, followed by a softmax operation to normalize these scores. This allows the model to focus on the most relevant tokens in the sequence when forming a representation for the current token.\n",
        "\n",
        "**Multi-head attention** extends this concept by allowing the model to use multiple sets of queries, keys, and values, learning different aspects of the relationships between tokens. This enables the model to focus on different patterns or \"heads\" of attention.\n",
        "\n",
        "### 3. **Transformers**\n",
        "\n",
        "Transformers are a deep learning architecture that relies heavily on the self-attention mechanism. Introduced by Vaswani et al. in 2017 with the paper *\"Attention is All You Need,\"* transformers have since become the foundation for most state-of-the-art NLP models like **BERT**, **GPT**, and **T5**.\n",
        "\n",
        "The transformer architecture typically consists of two parts:\n",
        "- **Encoder**: Processes the input sequence.\n",
        "- **Decoder**: Generates the output sequence (for tasks like translation or text generation).\n",
        "\n",
        "The key innovations of transformers compared to earlier models (like RNNs or LSTMs) include:\n",
        "\n",
        "- **Self-attention**: As discussed, this allows the model to look at the entire sequence at once and decide which parts are relevant for processing each token.\n",
        "  \n",
        "- **Positional encoding**: Since transformers do not have an inherent sense of token order (like RNNs do), they rely on positional encodings added to the embeddings to preserve information about the position of words in the sequence.\n",
        "\n",
        "- **Layer normalization and feedforward layers**: After self-attention, each token representation is processed through additional layers for further transformation and refinement.\n",
        "\n",
        "A transformer consists of multiple layers of self-attention and feedforward neural networks, with **multi-head attention** enabling the model to capture multiple relationships between tokens at different levels of abstraction.\n",
        "\n",
        "### How These Concepts Relate in the Transformer Pipeline:\n",
        "\n",
        "1. **Embeddings as Input**: The transformer starts by converting input tokens into embeddings (e.g., word or sub-word embeddings). These embeddings can be pre-trained using techniques like Word2Vec or contextual embeddings from BERT-like models. Positional encodings are then added to the embeddings to introduce order information.\n",
        "  \n",
        "2. **Self-Attention Mechanism**: The input embeddings are passed through multiple self-attention layers, allowing each token to gather information from all other tokens in the sequence. Multi-head attention enables the model to learn different relationships between tokens.\n",
        "\n",
        "3. **Contextual Embeddings as Output**: The output of a transformer layer is a **contextual embedding** for each token, meaning each token’s vector is updated based on its relationship with all the other tokens. These embeddings are used for downstream tasks like text classification, translation, or generating new text.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "- **Embeddings** (like Word2Vec) provide a way to represent tokens in a continuous vector space. In transformers, they serve as the starting input representation.\n",
        "- **Self-attention** enables the model to dynamically focus on the most relevant parts of the sequence when processing each token. It is a flexible and scalable alternative to the sequential nature of RNNs and LSTMs.\n",
        "- **Transformers** integrate embeddings, self-attention, and multi-head attention into a robust architecture that processes sequences in parallel, allowing for efficient learning of complex relationships within data. This has made transformers the dominant model in NLP tasks and beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120dab1b-8064-4b44-ac3a-4e74c2facae8",
      "metadata": {
        "id": "120dab1b-8064-4b44-ac3a-4e74c2facae8"
      },
      "source": [
        "## Transformer Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9232ad-a70a-4fec-873b-15ea751c3d0b",
      "metadata": {
        "id": "0c9232ad-a70a-4fec-873b-15ea751c3d0b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d086ac3-ccfa-4285-85d4-997a9ea82f94",
      "metadata": {
        "id": "0d086ac3-ccfa-4285-85d4-997a9ea82f94"
      },
      "outputs": [],
      "source": [
        "# Define a simple Transformer Encoder layer class\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the Transformer encoder layer.\n",
        "        - embed_dim: Dimension of the embedding space.\n",
        "        - num_heads: Number of attention heads.\n",
        "        - ff_dim: Hidden layer size in the feed-forward network.\n",
        "        - rate: Dropout rate to prevent overfitting.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Define the multi-head attention layer\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        # Define the feed-forward network: a two-layer MLP (Dense layers)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            # First dense layer with ReLU activation\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            # Second dense layer outputting the same dimensions as the input\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "\n",
        "        # Define layer normalization to stabilize training\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Define dropout layers to prevent overfitting\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass for the Transformer encoder.\n",
        "        - inputs: Input to the transformer encoder layer.\n",
        "        - training: Whether the layer is in training mode\n",
        "               (dropout applied) or inference mode.\n",
        "        \"\"\"\n",
        "        # Apply multi-head attention to the inputs (self-attention)\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # Add and normalize (residual connection and layer normalization)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # Add and normalize (residual connection and layer normalization)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "506b1021-14a8-42c3-a7f8-3e2d6c482df3",
      "metadata": {
        "id": "506b1021-14a8-42c3-a7f8-3e2d6c482df3"
      },
      "source": [
        "## Transformer Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a161f0f-5b45-4064-b103-4a353e6df352",
      "metadata": {
        "id": "8a161f0f-5b45-4064-b103-4a353e6df352"
      },
      "outputs": [],
      "source": [
        "# Define a Transformer-based text classification model\n",
        "def create_transformer_model(input_shape, embed_dim,\n",
        "                             num_heads, ff_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Create a Transformer-based classification model.\n",
        "    - input_shape: Shape of the input data\n",
        "        (number of tokens in each sequence).\n",
        "    - embed_dim: Dimension of the embedding.\n",
        "    - num_heads: Number of attention heads in the Transformer encoder.\n",
        "    - ff_dim: Feed-forward network dimension.\n",
        "    - num_classes: Number of output classes for classification.\n",
        "    \"\"\"\n",
        "    # Define the input layer. Expect sequences of integers (token IDs)\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Embed the input tokens using an embedding layer\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=embed_dim)(inputs)\n",
        "\n",
        "    # Pass the embeddings through the Transformer encoder layer\n",
        "    x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Apply global average pooling to reduce the sequence to a\n",
        "    # fixed size (averaging across tokens)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Add a dense output layer with softmax activation for classification\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # Create the Keras model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaa66ced-84c9-4d90-acb4-0cfa1045eb7d",
      "metadata": {
        "id": "aaa66ced-84c9-4d90-acb4-0cfa1045eb7d"
      },
      "source": [
        "## Transformer Application"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a94171-dfae-4a7f-80a9-22e94466488f",
      "metadata": {
        "id": "02a94171-dfae-4a7f-80a9-22e94466488f"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e13b7e-4d0a-49b4-822e-1e881c09fe6b",
      "metadata": {
        "id": "d5e13b7e-4d0a-49b4-822e-1e881c09fe6b"
      },
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "embed_dim = 64  # Size of the token embeddings\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 128  # Hidden layer size in the feed-forward network\n",
        "num_classes = 2  # Number of output classes (for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ed19ca-561f-4353-bf62-0607b44877bb",
      "metadata": {
        "id": "91ed19ca-561f-4353-bf62-0607b44877bb"
      },
      "outputs": [],
      "source": [
        "# Create the model using the function defined above\n",
        "model = create_transformer_model(input_shape=(100,),\n",
        "            embed_dim=embed_dim, num_heads=num_heads,\n",
        "            ff_dim=ff_dim, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b9c07a-bf2c-4f1b-bd27-7a3d52e842c9",
      "metadata": {
        "id": "32b9c07a-bf2c-4f1b-bd27-7a3d52e842c9"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer,\n",
        "# sparse categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7513450-b04d-4f23-b4fe-959574145b41",
      "metadata": {
        "id": "b7513450-b04d-4f23-b4fe-959574145b41"
      },
      "outputs": [],
      "source": [
        "# Print the model summary to visualize the architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4389810b-450a-458b-8332-6a1f87b8a8bd",
      "metadata": {
        "id": "4389810b-450a-458b-8332-6a1f87b8a8bd"
      },
      "outputs": [],
      "source": [
        "# Generate random input data (100 sequences of 100 tokens)\n",
        "# -> numerical data, typically tokenized from text\n",
        "X_train = np.random.randint(0, 5000, size=(100, 100))\n",
        "y_train = np.random.randint(0, 2, size=(100,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac83313-c59d-42a3-b43a-1038a0bd08eb",
      "metadata": {
        "id": "eac83313-c59d-42a3-b43a-1038a0bd08eb"
      },
      "outputs": [],
      "source": [
        "X_train[:10, :10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c4a7cc-f91d-4b7f-9017-ba825cc261ee",
      "metadata": {
        "id": "34c4a7cc-f91d-4b7f-9017-ba825cc261ee"
      },
      "outputs": [],
      "source": [
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52ef7b3-6b34-4e4b-97a8-79f2dc188f5f",
      "metadata": {
        "id": "c52ef7b3-6b34-4e4b-97a8-79f2dc188f5f"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "%time model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84d0889-72dd-40e2-9976-19f9803f163a",
      "metadata": {
        "id": "e84d0889-72dd-40e2-9976-19f9803f163a"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cab6ac3-c5e7-4c68-b7ff-a6c6c63942c1",
      "metadata": {
        "id": "9cab6ac3-c5e7-4c68-b7ff-a6c6c63942c1"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bee0e71-a7b8-4044-9f5a-ad93fc5d2aa9",
      "metadata": {
        "id": "7bee0e71-a7b8-4044-9f5a-ad93fc5d2aa9"
      },
      "outputs": [],
      "source": [
        "# Sample input text (numerical data, typically tokenized from text)\n",
        "sample_input = np.random.randint(0, 5000, size=(1, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4501f00e-b992-48bc-b35d-d8275de51f45",
      "metadata": {
        "id": "4501f00e-b992-48bc-b35d-d8275de51f45"
      },
      "outputs": [],
      "source": [
        "sample_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d4e2ba-53a1-4c70-8d0f-fe7513519102",
      "metadata": {
        "id": "11d4e2ba-53a1-4c70-8d0f-fe7513519102"
      },
      "outputs": [],
      "source": [
        "# Predict the class\n",
        "prediction = model.predict(sample_input)\n",
        "predicted_class = np.argmax(prediction, axis=1)\n",
        "\n",
        "print(f\"Predicted class: {predicted_class[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
      "metadata": {
        "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55"
      },
      "source": [
        "<img src=\"https://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
        "\n",
        "<a href=\"https://tpq.io\" target=\"_blank\">https://tpq.io</a> | <a href=\"https://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}