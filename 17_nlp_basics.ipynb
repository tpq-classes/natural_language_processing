{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
   "metadata": {},
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "**Word Embeddings**\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch\n",
    "\n",
    "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d071354-f0e1-4bf8-9917-020d62a000d0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/tpq-classes/natural_language_processing.git\n",
    "import sys\n",
    "sys.path.append('natural_language_processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032f0cb-8e91-409a-9c62-6839055c64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c90cc-8970-4e6e-9e0e-6a2f6f655ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaac2e-64c6-4733-abeb-c7d07558955d",
   "metadata": {},
   "source": [
    "## Text Classification (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470035df-27ee-4dfa-9053-ecabd52adb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_snippets = [\n",
    "    \"Python is a versatile language for web development, data analysis, and automation.\",\n",
    "    \"Use Python's libraries like NumPy and Pandas for efficient data manipulation.\",\n",
    "    \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n",
    "    \"The Python community offers extensive documentation and a wealth of online resources.\",\n",
    "    \"Python's syntax is designed to be readable and straightforward, making it beginner-friendly.\",\n",
    "    \"Django and Flask are popular frameworks for developing web applications in Python.\",\n",
    "    \"Automate repetitive tasks with Python scripts and save time in your workflow.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa1b83-d82e-4cb2-bfc5-fb28e8ba8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_snippets = [\n",
    "    \"Natural Language Processing (NLP) enables computers to understand and process human language.\",\n",
    "    \"NLP is used in applications like sentiment analysis, chatbots, and machine translation.\",\n",
    "    \"Tokenization is a fundamental step in NLP, breaking text into meaningful units.\",\n",
    "    \"Named Entity Recognition (NER) identifies proper nouns in text, such as names and locations.\",\n",
    "    \"Vectorization converts text data into numerical form for machine learning models.\",\n",
    "    \"Popular NLP libraries include NLTK, SpaCy, and Hugging Face Transformers.\",\n",
    "    \"NLP combines computational linguistics and machine learning for language understanding.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bbeee-da45-4967-8beb-e4380355f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_snippets = [\n",
    "    \"Large Language Models (LLMs) are advanced neural networks trained on vast text corpora.\",\n",
    "    \"LLMs like GPT-3 generate human-like text based on input prompts.\",\n",
    "    \"Applications of LLMs include content creation, code generation, and conversational agents.\",\n",
    "    \"LLMs utilize transformers, a deep learning architecture, for efficient processing.\",\n",
    "    \"Training LLMs requires substantial computational resources and large datasets.\",\n",
    "    \"Fine-tuning LLMs on specific tasks enhances their performance and accuracy.\",\n",
    "    \"Ethical considerations in LLMs include bias, misinformation, and data privacy.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f75d3-b6cb-4138-b9ec-fc031757394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list()\n",
    "X.extend(python_snippets)\n",
    "X.extend(nlp_snippets)\n",
    "X.extend(llm_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5d86a-18a1-4c29-bd68-3e39e6263f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea35724-10c7-465d-a4fc-c13056e01baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(7 * [0] + 7 * [1] + 7 * [2])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387c3cf-ca38-4079-b107-326e3fa57085",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fcc51-8c90-455e-b2b3-6cd445af925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(X)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd053b0-0918-4c35-b3e1-4d3e892e7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afa94c-384b-4156-b109-e453a595416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acdc22-d1ec-468c-9b55-adf484c81b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85dc42-2a02-4da5-8c83-5eb5155b01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1)\n",
    "# model = MLPClassifier(hidden_layer_sizes=[128, 128],\n",
    "#                       max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281e45-a5fa-424b-871a-3733da91bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086ddb4-881b-4e1a-9f77-5ac494a00391",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(X_)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a0872-04eb-4ab5-8a18-38a47ec3b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4f5e1-5524-4657-801f-35c4723cf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test snippets\n",
    "test_snippets = [\n",
    "    \"Python's extensive standard library supports many common programming tasks.\",\n",
    "    \"Jupyter notebooks are widely used for interactive Python development and data visualization.\",\n",
    "    \"Python's dynamic typing and garbage collection simplify memory management.\",\n",
    "    \"Sentiment analysis in NLP determines the emotional tone of text.\",\n",
    "    \"Text classification categorizes text into predefined labels using NLP techniques.\",\n",
    "    \"Word embeddings represent words as dense vectors for better machine learning performance.\",\n",
    "    \"Transfer learning is often used in LLMs to adapt pre-trained models to new tasks.\",\n",
    "    \"LLMs can summarize long documents, extracting key information efficiently.\",\n",
    "    \"Prompt engineering tailors inputs to guide LLM outputs more effectively.\"\n",
    "]\n",
    "\n",
    "# Labels for the test snippets\n",
    "new_labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f313a7-5ce1-4333-8acf-30d8b382866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vectorizer.transform(test_snippets)\n",
    "tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbca920-304d-4002-874a-274c772db0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = model.predict(tfidf_test)\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82e973-05d3-43cb-a74c-7a4f7ceb8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(new_labels, p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380ab80-ac7d-4130-800d-8946196dbcae",
   "metadata": {},
   "source": [
    "## Text Classification (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56800507-6692-4e8e-afb6-04327181ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecad069-fb4e-4808-a318-e6ee0f4ad573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(categories=['sci.med', 'sci.crypt', 'sci.space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe26745-7716-4588-8fa5-39a5f2af5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694358be-dd0e-4706-8822-48af3fe0cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa7956-7d39-4f18-8c7f-e1088d32a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a34702-776a-432b-b836-2262c9835e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a3ff5-4353-4514-b853-7ea8a427d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(stop_words='english'),\n",
    "                         LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101a9e2-be2d-4580-8e10-c3bf44071d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df03959-55a1-494a-8a50-5c8cf3a64756",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = pipeline.predict(X_train)\n",
    "p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b87ce-0af4-4ae4-84fa-a890fcc46a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, p_train)  # in-sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6875033-b6e8-4e2e-9f18-aa46493416ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2df572-e21c-48ae-be38-3053221a5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e141503-4d0c-42fd-8306-61920d5ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71947723-9329-4927-845a-292c7df10b87",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f0f7d-cf70-4a49-b706-8afcac0255a1",
   "metadata": {},
   "source": [
    "_From ChatGPT._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2b28b-a304-45f5-b914-5eabfed9789b",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique used in natural language processing (NLP) to create word embeddings, which are dense vector representations of words. Developed by a team of researchers at Google led by Tomas Mikolov in 2013, Word2Vec models learn to map words into high-dimensional continuous vector spaces where semantically similar words are located close to each other.\n",
    "\n",
    "### Key Concepts of Word2Vec\n",
    "\n",
    "1. **Distributed Representations**: Unlike traditional one-hot encoding, which represents words as sparse vectors with many dimensions (one per unique word) and no meaningful distances between them, Word2Vec creates dense vectors where the semantic relationships between words are captured in the vector space.\n",
    "\n",
    "2. **Training Approaches**:\n",
    "   - **Continuous Bag of Words (CBOW)**: Predicts the target word based on the context of surrounding words. It uses a window of words around the target word to predict the target word itself.\n",
    "   - **Skip-gram**: Predicts the surrounding context words based on the target word. Given a word, it tries to predict the words in its neighborhood.\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "1. **Input**: A large corpus of text.\n",
    "2. **Training**: The model is trained on the corpus to predict context words from a target word (skip-gram) or a target word from context words (CBOW).\n",
    "3. **Output**: A set of word vectors where each word is represented by a dense vector of real numbers.\n",
    "\n",
    "### Benefits of Word2Vec\n",
    "\n",
    "- **Semantic Relationships**: Words with similar meanings are close together in the vector space. For example, \"king\" - \"man\" + \"woman\" is close to \"queen\".\n",
    "- **Efficient**: Word2Vec can be trained on large corpora efficiently using stochastic gradient descent and other optimization techniques.\n",
    "- **Generalization**: The vectors can be used in various downstream NLP tasks, improving their performance by providing meaningful word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0fb4e-f4d3-4806-b425-e5f93ad80895",
   "metadata": {},
   "source": [
    "### Example in Python using Gensim\n",
    "\n",
    "Here's how you can use the Gensim library to create Word2Vec embeddings:\n",
    "\n",
    "1. **Install Gensim**:\n",
    "   ```bash\n",
    "   pip install gensim\n",
    "   ```\n",
    "\n",
    "2. **Train a Word2Vec Model**:\n",
    "   ```python\n",
    "   from gensim.models import Word2Vec\n",
    "\n",
    "   # Sample sentences\n",
    "   sentences = [\n",
    "       [\"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "       [\"dog\", \"barked\", \"at\", \"the\", \"mailman\"],\n",
    "       [\"fish\", \"swims\", \"in\", \"the\", \"water\"]\n",
    "   ]\n",
    "\n",
    "   # Train Word2Vec model\n",
    "   model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "   # Get embeddings for a word\n",
    "   cat_vector = model.wv['cat']\n",
    "   print(\"Embedding for 'cat':\", cat_vector)\n",
    "   \n",
    "   # Find similar words\n",
    "   similar_to_cat = model.wv.most_similar('cat')\n",
    "   print(\"Words similar to 'cat':\", similar_to_cat)\n",
    "   ```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Training Data**: We use a small set of sample sentences.\n",
    "- **Word2Vec Model**: We create a Word2Vec model using these sentences.\n",
    "  - `vector_size`: The size of the word vectors.\n",
    "  - `window`: The maximum distance between the current and predicted word within a sentence.\n",
    "  - `min_count`: Ignores all words with a total frequency lower than this.\n",
    "  - `sg`: Training algorithm, 0 for CBOW (Continuous Bag of Words), and 1 for skip-gram.\n",
    "- **Get Embeddings**: We retrieve the embeddings for specific words like 'cat'.\n",
    "- **Find Similar Words**: We find words similar to 'cat' based on the trained embeddings.\n",
    "\n",
    "### Applications of Word2Vec\n",
    "\n",
    "1. **Text Classification**: Improved feature representations lead to better classification performance.\n",
    "2. **Semantic Analysis**: Understanding relationships between words.\n",
    "3. **Machine Translation**: Capturing the semantic meaning of words helps in translating sentences more accurately.\n",
    "4. **Recommendation Systems**: Finding similar items or content based on word embeddings.\n",
    "5. **Information Retrieval**: Improving search results by understanding the semantic context of queries.\n",
    "\n",
    "Word2Vec has been a foundational technique in NLP, leading to more advanced embedding methods like GloVe, FastText, and contextual embeddings such as BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
   "metadata": {},
   "source": [
    "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
    "\n",
    "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}