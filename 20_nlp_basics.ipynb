{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
      "metadata": {
        "id": "475819a4-e148-4616-b1cb-44b659aeb08a"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
      "metadata": {
        "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e"
      },
      "source": [
        "# NLP Basics\n",
        "\n",
        "**Transformers**\n",
        "\n",
        "&copy; Dr. Yves J. Hilpisch\n",
        "\n",
        "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d071354-f0e1-4bf8-9917-020d62a000d0",
      "metadata": {
        "id": "3d071354-f0e1-4bf8-9917-020d62a000d0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHf6cP-xl2wx"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!git clone https://github.com/tpq-classes/natural_language_processing.git\n",
        "import sys\n",
        "sys.path.append('natural_language_processing')\n"
      ],
      "id": "MHf6cP-xl2wx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "TA5qSXgzmLnm"
      },
      "id": "TA5qSXgzmLnm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4032f0cb-8e91-409a-9c62-6839055c64bc",
      "metadata": {
        "id": "4032f0cb-8e91-409a-9c62-6839055c64bc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9380ab80-ac7d-4130-800d-8946196dbcae",
      "metadata": {
        "id": "9380ab80-ac7d-4130-800d-8946196dbcae"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6be8df1-5b3f-4822-8ca8-2ec44e8778d8",
      "metadata": {
        "id": "b6be8df1-5b3f-4822-8ca8-2ec44e8778d8"
      },
      "source": [
        "Transformer models are dating back to the seminal paper \"Attention Is All You Need\":\n",
        "\n",
        "https://arxiv.org/abs/1706.03762\n",
        "\n",
        "_From ChatGPT_:\n",
        "\n",
        "Transformers in Natural Language Processing (NLP) are a type of neural network architecture designed to process sequences of data, like text, by focusing on the relationship between words in a sentence. They use something called self-attention to understand how different words in a sentence are related to each other, regardless of their position. This allows them to capture the context of words in a way that previous models like RNNs or LSTMs struggled with.\n",
        "\n",
        "**Key Concept: Self-Attention**\n",
        "\n",
        "Self-attention allows the model to weigh the importance of different words when analyzing each word in a sentence. For example, in the sentence \"The cat sat on the mat,\" the word \"mat\" is more related to \"sat\" than to \"the.\" The transformer can focus on those important relationships even if the sentence becomes longer.\n",
        "\n",
        "**Transformer Architecture Overview (High Level):**\n",
        "\n",
        "Encoder: It reads the input sentence and creates a representation of each word by looking at the entire sentence (using self-attention).\n",
        "\n",
        "Decoder: It processes this representation to generate an output (in tasks like machine translation).\n",
        "\n",
        "Self-Attention: This mechanism helps the transformer model decide which words in the sentence are most important for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5847a08d-fe07-4669-8c83-aa66a106d9d1",
      "metadata": {
        "id": "5847a08d-fe07-4669-8c83-aa66a106d9d1"
      },
      "source": [
        "## Simple Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e199ab-2b62-4247-b8b7-23ab38994388",
      "metadata": {
        "id": "64e199ab-2b62-4247-b8b7-23ab38994388"
      },
      "source": [
        "Example input: a simple sentence represented by 3 words (each word is a vector). Let's assume each word is represented by a vector of 4 values (features). In practice, these are embeddings, but we'll use random vectors here for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "663536c8-718a-4e33-817e-410ad9e92af5",
      "metadata": {
        "id": "663536c8-718a-4e33-817e-410ad9e92af5"
      },
      "outputs": [],
      "source": [
        "sentence = np.array([[1, 0, 1, 0],    # Word 1\n",
        "                     [0, 2, 0, 2],    # Word 2\n",
        "                     [1, 1, 1, 1]])   # Word 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6067484-b9d0-4965-86f3-a62d7180c53d",
      "metadata": {
        "id": "b6067484-b9d0-4965-86f3-a62d7180c53d"
      },
      "source": [
        "Step 1: Compute the dot product of the sentence matrix with itself (transpose) to get attention scores between words. This gives us a matrix where each element (i, j) represents the \"importance\" of word i with respect to word j."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43a9498-52ed-4d8f-935f-6282d603dbeb",
      "metadata": {
        "id": "c43a9498-52ed-4d8f-935f-6282d603dbeb"
      },
      "outputs": [],
      "source": [
        "attention_scores = np.dot(sentence, sentence.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a2be61-0fbc-483f-862c-b08877166150",
      "metadata": {
        "id": "d2a2be61-0fbc-483f-862c-b08877166150"
      },
      "outputs": [],
      "source": [
        "attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f538b5c-b956-4d7d-9ce4-d03c54d1c515",
      "metadata": {
        "id": "6f538b5c-b956-4d7d-9ce4-d03c54d1c515"
      },
      "source": [
        "Step 2: Normalize the attention scores using softmax to get the attention weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5c8df5-715f-4f63-9c0a-6c426a9d237d",
      "metadata": {
        "id": "2c5c8df5-715f-4f63-9c0a-6c426a9d237d"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1ad7fa-6d5e-4ca5-887f-6e572342ea16",
      "metadata": {
        "id": "1e1ad7fa-6d5e-4ca5-887f-6e572342ea16"
      },
      "outputs": [],
      "source": [
        "attention_weights = softmax(attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83841479-be5a-458b-a720-c4ae4fff77de",
      "metadata": {
        "id": "83841479-be5a-458b-a720-c4ae4fff77de"
      },
      "outputs": [],
      "source": [
        "attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9710c7a-236f-4880-81f9-e194da1052d0",
      "metadata": {
        "id": "c9710c7a-236f-4880-81f9-e194da1052d0"
      },
      "outputs": [],
      "source": [
        "attention_weights.sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fe2f49-6ec6-4110-8f4b-dc4835de6480",
      "metadata": {
        "id": "f1fe2f49-6ec6-4110-8f4b-dc4835de6480"
      },
      "source": [
        "Step 3: Multiply the attention weights by the original sentence vectors to create a new representation for each word based on the context of the entire sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b1c0e8-8a5e-400d-8d71-45a048163d42",
      "metadata": {
        "id": "f7b1c0e8-8a5e-400d-8d71-45a048163d42"
      },
      "outputs": [],
      "source": [
        "new_sentence_representation = np.dot(attention_weights, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad29f7a-1b81-4e65-ae41-247d2fed5781",
      "metadata": {
        "id": "4ad29f7a-1b81-4e65-ae41-247d2fed5781"
      },
      "outputs": [],
      "source": [
        "new_sentence_representation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894b2213-ccd8-42c1-8a2d-6644352c5ad6",
      "metadata": {
        "id": "894b2213-ccd8-42c1-8a2d-6644352c5ad6"
      },
      "source": [
        "**Why Self-Attention Is Powerful**\n",
        "\n",
        "_Context Awareness_: Each word's new representation is a combination of all other words in the sentence, meaning it has a sense of context, regardless of position.\n",
        "\n",
        "_Parallelization_: Unlike RNNs or LSTMs, transformers process all words simultaneously, making them much faster for long sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5850960f-2686-4808-902b-32870a38a49e",
      "metadata": {
        "id": "5850960f-2686-4808-902b-32870a38a49e"
      },
      "source": [
        "## Another Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a693a59-50cf-49a7-b8a4-56f9d61ed623",
      "metadata": {
        "id": "1a693a59-50cf-49a7-b8a4-56f9d61ed623"
      },
      "outputs": [],
      "source": [
        "# Define a simple sequence of 4 input vectors (each vector represents a word or token)\n",
        "# In this case, we are using 3-dimensional vectors for simplicity\n",
        "inputs = np.array([[1, 0, 1],    # Input 1\n",
        "                   [0, 1, 0],    # Input 2\n",
        "                   [1, 1, 1],    # Input 3\n",
        "                   [0, 0, 1]])   # Input 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13faf7b-c050-4330-b470-44033a263dfc",
      "metadata": {
        "id": "f13faf7b-c050-4330-b470-44033a263dfc"
      },
      "outputs": [],
      "source": [
        "# Step 1: Compute attention scores (similarity of each input with the others)\n",
        "# Here we'll compute the dot product of each input with a learnable attention vector (query)\n",
        "attention_vector = np.array([1, 1, 1])  # This can be learned during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb2786b-2261-4e56-879f-e4c7ff3a7e54",
      "metadata": {
        "id": "dcb2786b-2261-4e56-879f-e4c7ff3a7e54"
      },
      "outputs": [],
      "source": [
        "attention_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2499dd1-65f8-408f-800b-8bbe46f4443b",
      "metadata": {
        "id": "d2499dd1-65f8-408f-800b-8bbe46f4443b"
      },
      "outputs": [],
      "source": [
        "# Compute attention scores for each input by taking the dot product with the attention vector\n",
        "attention_scores = np.dot(inputs, attention_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c18a544f-65dc-4189-a1fd-0f7953fd1cab",
      "metadata": {
        "id": "c18a544f-65dc-4189-a1fd-0f7953fd1cab"
      },
      "outputs": [],
      "source": [
        "attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0dc446-5325-4714-8ff0-debd347e3c66",
      "metadata": {
        "id": "4a0dc446-5325-4714-8ff0-debd347e3c66"
      },
      "outputs": [],
      "source": [
        "# Step 2: Normalize the attention scores using softmax to get the attention weights\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a5dc55-d2b3-423e-905e-5548c14a50c9",
      "metadata": {
        "id": "78a5dc55-d2b3-423e-905e-5548c14a50c9"
      },
      "outputs": [],
      "source": [
        "attention_weights = softmax(attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d222a4-11ae-48ed-a392-469cced9e372",
      "metadata": {
        "id": "02d222a4-11ae-48ed-a392-469cced9e372"
      },
      "outputs": [],
      "source": [
        "attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d573967-d2f6-4401-8289-65aa322b4637",
      "metadata": {
        "id": "6d573967-d2f6-4401-8289-65aa322b4637"
      },
      "outputs": [],
      "source": [
        "attention_weights.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e0ef10-7e89-455e-84d6-91a30acf12d8",
      "metadata": {
        "id": "96e0ef10-7e89-455e-84d6-91a30acf12d8"
      },
      "outputs": [],
      "source": [
        "# Step 3: Compute the weighted sum of the inputs using the attention weights\n",
        "# This results in a context vector that captures the important information from the sequence\n",
        "context_vector = np.dot(attention_weights, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfe2251-d0d8-41c7-b20c-58c2c4cdde17",
      "metadata": {
        "id": "2bfe2251-d0d8-41c7-b20c-58c2c4cdde17"
      },
      "outputs": [],
      "source": [
        "context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754539ad-4135-4140-8760-55fedd908283",
      "metadata": {
        "id": "754539ad-4135-4140-8760-55fedd908283"
      },
      "outputs": [],
      "source": [
        "# Print the attention scores, weights, and the resulting context vector\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nAttention Scores:\\n\", attention_scores)\n",
        "print(\"\\nAttention Weights (after softmax):\\n\", attention_weights)\n",
        "print(\"\\nContext Vector (weighted sum of inputs):\\n\", context_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc42e49-da9f-4e32-b27d-dc70cb0b4a28",
      "metadata": {
        "id": "6bc42e49-da9f-4e32-b27d-dc70cb0b4a28"
      },
      "source": [
        "## TF-IDF Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15857463-d979-44f0-b89a-001c795cc982",
      "metadata": {
        "id": "15857463-d979-44f0-b89a-001c795cc982"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9d5e56-65eb-477d-b429-ac139b256c51",
      "metadata": {
        "id": "1a9d5e56-65eb-477d-b429-ac139b256c51"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a small set of documents (texts)\n",
        "documents = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog barked at the cat\",\n",
        "    \"The bird flew over the tree\",\n",
        "    \"The dog chased the bird\",\n",
        "    \"The cat climbed the tree\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6f1ae1-d883-4397-91db-def2f4e64dd2",
      "metadata": {
        "id": "0e6f1ae1-d883-4397-91db-def2f4e64dd2"
      },
      "outputs": [],
      "source": [
        "# Step 2: Train a TF-IDF model on the documents\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(documents).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735a02e2-ca90-4032-8327-8b42fb05cfdc",
      "metadata": {
        "id": "735a02e2-ca90-4032-8327-8b42fb05cfdc"
      },
      "outputs": [],
      "source": [
        "# Print the TF-IDF vectors for reference\n",
        "print(\"TF-IDF Vectors:\\n\", tfidf_vectors.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e571587c-575c-4aba-acf3-7b190f002edb",
      "metadata": {
        "id": "e571587c-575c-4aba-acf3-7b190f002edb"
      },
      "outputs": [],
      "source": [
        "# Step 3: Choose one document as the \"query\" document (we'll calculate attention relative to this)\n",
        "# Let's take the first document: \"The cat sat on the mat\"\n",
        "query_vector = tfidf_vectors[0].round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b23d33-cdc8-494a-b53a-632157ed61dd",
      "metadata": {
        "id": "b3b23d33-cdc8-494a-b53a-632157ed61dd"
      },
      "outputs": [],
      "source": [
        "query_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93c169c-0229-4407-b213-744e3c85261d",
      "metadata": {
        "id": "a93c169c-0229-4407-b213-744e3c85261d"
      },
      "outputs": [],
      "source": [
        "# Step 4: Calculate attention scores by computing the dot product between the query and other documents\n",
        "attention_scores = np.dot(tfidf_vectors, query_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0d42e5-cb58-4f40-baa1-d7c7e67ed446",
      "metadata": {
        "id": "2f0d42e5-cb58-4f40-baa1-d7c7e67ed446"
      },
      "outputs": [],
      "source": [
        "attention_scores.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a975cd59-cd26-49b4-9b40-9b7f9b23ffab",
      "metadata": {
        "id": "a975cd59-cd26-49b4-9b40-9b7f9b23ffab"
      },
      "outputs": [],
      "source": [
        "# Step 5: Normalize the attention scores using softmax to get the attention weights\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73b7da9-0880-4ad9-a04a-258361cc7723",
      "metadata": {
        "id": "c73b7da9-0880-4ad9-a04a-258361cc7723"
      },
      "outputs": [],
      "source": [
        "attention_weights = softmax(attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629830ed-b7f4-4d8b-a042-c14111dd7129",
      "metadata": {
        "id": "629830ed-b7f4-4d8b-a042-c14111dd7129"
      },
      "outputs": [],
      "source": [
        "attention_weights.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbf8f22-a2ec-4cf2-a6ad-5079343ecbfb",
      "metadata": {
        "id": "4bbf8f22-a2ec-4cf2-a6ad-5079343ecbfb"
      },
      "outputs": [],
      "source": [
        "attention_weights.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aceb1541-82f0-4d11-a800-5a0515db9773",
      "metadata": {
        "id": "aceb1541-82f0-4d11-a800-5a0515db9773"
      },
      "outputs": [],
      "source": [
        "# Step 6: Calculate the context vector as the weighted sum of all TF-IDF vectors\n",
        "context_vector = np.dot(attention_weights, tfidf_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1f2408-41ec-4710-9d3f-9a81515c3b6f",
      "metadata": {
        "id": "9d1f2408-41ec-4710-9d3f-9a81515c3b6f"
      },
      "outputs": [],
      "source": [
        "context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3b1ebe-da74-48e4-bbfd-20cfb2bfd1a4",
      "metadata": {
        "id": "0d3b1ebe-da74-48e4-bbfd-20cfb2bfd1a4"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "print(\"\\nAttention Scores:\\n\", attention_scores)\n",
        "print(\"\\nAttention Weights (after softmax):\\n\", attention_weights)\n",
        "print(\"\\nContext Vector (weighted sum of TF-IDF vectors):\\n\", context_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90bd5a5a-50a8-40a6-bb77-5a2f499ebe1d",
      "metadata": {
        "id": "90bd5a5a-50a8-40a6-bb77-5a2f499ebe1d"
      },
      "source": [
        "## Word2Vec Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c902154-2827-47f7-b21f-d8805e186e66",
      "metadata": {
        "id": "9c902154-2827-47f7-b21f-d8805e186e66"
      },
      "source": [
        "In this example, we'll use a pre-trained Word2Vec model from gensim. We will follow the same process as before:\n",
        "\n",
        "1. Load a pre-trained Word2Vec model.\n",
        "2. Convert a sentence into word embeddings using this model.\n",
        "3. Apply the self-attention mechanism on the Word2Vec embeddings to create new word representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d507a167-b94a-4662-8e0d-2f51fee70d33",
      "metadata": {
        "id": "d507a167-b94a-4662-8e0d-2f51fee70d33"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2da6e13-7ee8-48e2-a30e-7613b30a26a6",
      "metadata": {
        "id": "d2da6e13-7ee8-48e2-a30e-7613b30a26a6"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "# pprint(api.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a08fac-0b04-4c4c-bf1b-244de7ab4d90",
      "metadata": {
        "id": "a7a08fac-0b04-4c4c-bf1b-244de7ab4d90"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Word2Vec model from gensim\n",
        "word2vec_model = api.load('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece27765-2f93-4e37-b436-682204c131e7",
      "metadata": {
        "id": "ece27765-2f93-4e37-b436-682204c131e7"
      },
      "outputs": [],
      "source": [
        "similar_words = word2vec_model.most_similar('cat', topn=5)\n",
        "similar_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbdae2a-3a84-4891-a39f-1aec00b4cb16",
      "metadata": {
        "id": "fdbdae2a-3a84-4891-a39f-1aec00b4cb16"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a sentence and convert each word to its Word2Vec embedding\n",
        "sentence = 'The cat sat on the mat'.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577a9b8d-5ee3-4809-aa81-a660fe4b53ad",
      "metadata": {
        "id": "577a9b8d-5ee3-4809-aa81-a660fe4b53ad"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a sentence and convert each word to its Word2Vec embedding\n",
        "# sentence = 'The bird flew over the tree'.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3667da40-b4de-4251-9095-da119c6e91c1",
      "metadata": {
        "id": "3667da40-b4de-4251-9095-da119c6e91c1"
      },
      "outputs": [],
      "source": [
        "# Fetch Word2Vec embeddings for each word in the sentence\n",
        "# Note: If a word is not in the Word2Vec vocabulary, we can skip it or assign a zero vector\n",
        "word_vectors = []\n",
        "for word in sentence:\n",
        "    if word in word2vec_model:\n",
        "        word_vectors.append(word2vec_model[word])\n",
        "    else:\n",
        "        word_vectors.append(np.zeros(50))  # Assign a zero vector if word is not in vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc3e152-5214-46fd-b154-b950902a5e7b",
      "metadata": {
        "id": "bcc3e152-5214-46fd-b154-b950902a5e7b"
      },
      "outputs": [],
      "source": [
        "word_vectors = np.array(word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6311276-04c9-4df0-86b5-58af7cf129c2",
      "metadata": {
        "id": "c6311276-04c9-4df0-86b5-58af7cf129c2"
      },
      "outputs": [],
      "source": [
        "# word_vectors.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6385729a-521c-49cb-b885-3ee0d7e252d4",
      "metadata": {
        "id": "6385729a-521c-49cb-b885-3ee0d7e252d4"
      },
      "outputs": [],
      "source": [
        "# Step 2: Compute the dot product of the sentence matrix with itself (transpose)\n",
        "attention_scores = np.dot(word_vectors, word_vectors.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3182737e-8779-4666-b6eb-9f82cf9fe323",
      "metadata": {
        "id": "3182737e-8779-4666-b6eb-9f82cf9fe323"
      },
      "outputs": [],
      "source": [
        "attention_scores.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b77d70e-4de8-42c9-a2e8-14b7c7e3e7bc",
      "metadata": {
        "id": "0b77d70e-4de8-42c9-a2e8-14b7c7e3e7bc"
      },
      "outputs": [],
      "source": [
        "# Step 3: Normalize the attention scores using softmax\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61af4d35-7206-424d-ad59-a5c9e24871d6",
      "metadata": {
        "id": "61af4d35-7206-424d-ad59-a5c9e24871d6"
      },
      "outputs": [],
      "source": [
        "attention_weights = softmax(attention_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9520186c-a558-46c0-a9bc-ceec686b92d8",
      "metadata": {
        "id": "9520186c-a558-46c0-a9bc-ceec686b92d8"
      },
      "outputs": [],
      "source": [
        "attention_weights.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e405facb-12b1-44d3-aae8-83a8d3baea57",
      "metadata": {
        "id": "e405facb-12b1-44d3-aae8-83a8d3baea57"
      },
      "outputs": [],
      "source": [
        "attention_weights.sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e0dd3b-7827-4f8d-bff3-7de1a9df45c1",
      "metadata": {
        "id": "42e0dd3b-7827-4f8d-bff3-7de1a9df45c1"
      },
      "outputs": [],
      "source": [
        "# Step 4: Multiply the attention weights by the original sentence vectors\n",
        "new_sentence_representation = np.dot(attention_weights, word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee64021f-ca42-446d-9576-e11b7541a1e6",
      "metadata": {
        "id": "ee64021f-ca42-446d-9576-e11b7541a1e6"
      },
      "outputs": [],
      "source": [
        "# new_sentence_representation.round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7785226-2ef1-4925-9276-7cb647f93f6c",
      "metadata": {
        "id": "e7785226-2ef1-4925-9276-7cb647f93f6c"
      },
      "outputs": [],
      "source": [
        "# Step 5: Show the original word vectors and the new word vectors after self-attention\n",
        "print(f\"Original sentence: {' '.join(sentence)}\")\n",
        "print(\"\\nAttention Weights (after softmax):\\n\", attention_weights)\n",
        "\n",
        "print(\"\\nNew Sentence Representation (first 5 values of each vector):\")\n",
        "for i, word in enumerate(sentence):\n",
        "    print(f\"New vector for '{word}': {new_sentence_representation[i][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
      "metadata": {
        "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
        "\n",
        "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}